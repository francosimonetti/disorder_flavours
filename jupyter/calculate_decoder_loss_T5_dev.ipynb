{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "\t- Output attentions: False\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import json\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "os.chdir(\"/biodata/franco/zsuzsa_lab\")\n",
    "\n",
    "def sequence_masker(seq, i, j, same_extra_token=False):\n",
    "    masked_sequence_list = seq.split()\n",
    "    token_num = 0\n",
    "    if j<=i:\n",
    "        print(f\"index j={j} must be greater than i={i}\")\n",
    "        raise\n",
    "    for x in range(i, j):\n",
    "        if j > len(seq):\n",
    "            break\n",
    "        masked_sequence_list[x] = f\"<extra_id_{token_num}>\"\n",
    "        if not same_extra_token:\n",
    "            token_num += 1\n",
    "    return \" \".join(masked_sequence_list)\n",
    "\n",
    "output_attentions = False\n",
    "autoregressive = False\n",
    "savelogits = False\n",
    "outdir = \"test_T5_results_dev\"\n",
    "fastafile = \"jupyter/human_proteome_600_to_1200aa.fasta\"\n",
    "\n",
    "\n",
    "print(\"Parameters:\")\n",
    "print(f\"\\t- Output attentions: {output_attentions}\")\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "modelpath = \"models/prottrans_t5_xl_u50/\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "fullmodel = T5ForConditionalGeneration.from_pretrained(modelpath).to(device)\n",
    "celoss = CrossEntropyLoss()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(modelpath, do_lower_case=False, legacy=False)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "os.makedirs(f\"{outdir}/attentions\", exist_ok=True)\n",
    "os.makedirs(f\"{outdir}/logits\", exist_ok=True)\n",
    "# Read fasta sequences\n",
    "counter = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = 1\n",
    "end = 2\n",
    "start_time = time.time()\n",
    "\n",
    "total_iterations = 0\n",
    "\n",
    "for record in SeqIO.parse(fastafile, \"fasta\"):\n",
    "    #sequences.append(record)\n",
    "    if counter >= start and counter < end:\n",
    "        uniprot_id = record.id\n",
    "        aa_sequence = str(record.seq)\n",
    "        \n",
    "        pred_dict = dict()\n",
    "        mask_sizes = [1]\n",
    "        print(f\" Processing {uniprot_id}, protnum {counter}\")\n",
    "        if uniprot_id not in pred_dict:\n",
    "            pred_dict[uniprot_id] = dict()\n",
    "        \n",
    "        target_seq = aa_sequence\n",
    "        input_seq = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", target_seq)))]\n",
    "        true_input = tokenizer(input_seq)\n",
    "        true_tok = torch.tensor(true_input['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(true_input['attention_mask']).to(device)\n",
    "        if not os.path.exists(f\"{outdir}/{uniprot_id}.json\"):\n",
    "            for mask_size in mask_sizes:\n",
    "                print(f\"#### Mask size: {mask_size} ####\")\n",
    "                    \n",
    "                loss_sequence = list()\n",
    "                match_sequence = list()\n",
    "                logits_sequence = list()\n",
    "                for i in tqdm(range(len(target_seq)-mask_size+1)):\n",
    "                    masked_seq = [sequence_masker(input_seq[0], i, i+mask_size)]\n",
    "                    tmp = tokenizer(masked_seq)\n",
    "                    input_ids = torch.tensor(tmp['input_ids']).to(device)\n",
    "                    \n",
    "                    if autoregressive:\n",
    "                        with torch.no_grad():\n",
    "                            emb = fullmodel.generate(input_ids, max_length=len(input_ids[0])+1, return_dict_in_generate=True, output_scores=True, output_attentions=output_attentions)\n",
    "                        cpulogits = torch.stack([s.squeeze() for s in emb.scores], dim=0)[:-1,]\n",
    "                        fastpred = tokenizer.decode(torch.argmax(cpulogits, dim=1), skip_special_tokens=False).replace(\"<\",\" <\").replace(\">\",\"> \")\n",
    "                        \n",
    "                        # Calculate protein-level loss from single aminoacid losses\n",
    "                        acum = 0\n",
    "                        for j in range(cpulogits.shape[0]):\n",
    "                            acum += celoss(cpulogits[j], true_tok.squeeze()[:-1][j])        \n",
    "                        protein_loss  = acum/cpulogits.shape[0]\n",
    "                        # the proper full protein loss should be the one below, but it will fail if dimentions are not correct\n",
    "                        # the above expression will be wrong, but aproximate? maybe shifted by one? it will suck, let's set it to -1\n",
    "                        ## protein_loss2 = celoss(cpulogits, true_tok.squeeze()[:-1])\n",
    "                            \n",
    "                    else:\n",
    "                        with torch.no_grad():\n",
    "                            emb = fullmodel(input_ids=input_ids, labels=true_tok, output_attentions=output_attentions)\n",
    "                        protein_loss = emb.loss.cpu()\n",
    "                        \n",
    "                        cpulogits = emb.logits.cpu().squeeze()[:-1]\n",
    "                        ## fastpred = tokenizer.decode(torch.tensor(cpulogits[:,:-1,:].numpy().argmax(-1)[0]), skip_special_tokens=False).replace(\"<\",\" <\").replace(\">\",\"> \")\n",
    "                        fastpred = tokenizer.decode(torch.argmax(cpulogits, dim=1), skip_special_tokens=False).replace(\"<\",\" <\").replace(\">\",\"> \")\n",
    "                        \n",
    "                    logits_sequence.append(cpulogits.numpy().tolist())\n",
    "                    if input_seq[0] == fastpred:\n",
    "                        match_sequence.append(True)\n",
    "                        loss_sequence.append(protein_loss.item())\n",
    "                    else:\n",
    "                        pred_arr = fastpred.split()\n",
    "                        seq_arr  = input_seq[0].split()\n",
    "                        if len(pred_arr) == len(seq_arr):\n",
    "                            local_match_sequence = list()\n",
    "                            for j in range(len(pred_arr)):\n",
    "                                if pred_arr[j] != seq_arr[j]:\n",
    "                                    local_match_sequence.append((j,pred_arr[j], seq_arr[j]))\n",
    "                            loss_sequence.append(protein_loss.item())\n",
    "                            match_sequence.append(local_match_sequence)\n",
    "                        else:\n",
    "                            print(f\"{i} - Mismatch length error\")\n",
    "                            match_sequence.append(False)\n",
    "                            loss_sequence.append(-1)\n",
    "\n",
    "                    total_iterations += 1                        \n",
    "                        \n",
    "                pred_dict[uniprot_id][f\"aamask_{mask_size}\"] = dict()\n",
    "                pred_dict[uniprot_id][f\"aamask_{mask_size}\"][\"match\"] = match_sequence\n",
    "                pred_dict[uniprot_id][f\"aamask_{mask_size}\"][\"loss\"] = loss_sequence\n",
    "                ### This takes too much time and space, we will save the logits somewhere else\n",
    "                # pred_dict[uniprot_id][f\"aamask_{mask_size}\"][\"logits\"] = logits_sequence\n",
    "                np.save(f\"{outdir}/logits/{uniprot_id}_logits_sequence.npy\",  np.array(logits_sequence, dtype=object), allow_pickle=True)\n",
    "            with open(f\"{outdir}/{uniprot_id}.json\", 'w') as outfmt:\n",
    "                json.dump(pred_dict, outfmt)\n",
    "        else:\n",
    "            print(f\"Skipping {uniprot_id} masks\")\n",
    "        if savelogits:\n",
    "            if not os.path.exists(f\"{outdir}/logits/{uniprot_id}_logits.pt\"):\n",
    "                ## Output the complete attention matrices with a full pass, no masked aminoacids\n",
    "                with torch.no_grad():\n",
    "                    if autoregressive:\n",
    "                        emb = fullmodel.generate(true_tok, max_length=len(true_tok[0])+1, return_dict_in_generate=True, output_scores=True, output_attentions=output_attentions)\n",
    "                        cpulogits = torch.stack([s.squeeze() for s in emb.scores], dim=0)[:-1,].cpu()\n",
    "                        torch.save(cpulogits, f\"{outdir}/logits/{uniprot_id}_logits.pt\")\n",
    "                    else:\n",
    "                        emb = fullmodel(input_ids=true_tok, labels=true_tok, output_attentions=output_attentions)\n",
    "                        cpulogits = emb.logits.squeeze()[:-1].cpu()\n",
    "                        torch.save(cpulogits, f\"{outdir}/logits/{uniprot_id}_logits.pt\")\n",
    "                if output_attentions:\n",
    "                    os.makedirs(f\"{outdir}/attentions\", exist_ok=True)\n",
    "                    torch.save(emb.encoder_attentions, f\"{outdir}/attentions/{uniprot_id}_encoder_attentions.pt\")\n",
    "                    torch.save(emb.decoder_attentions, f\"{outdir}/attentions/{uniprot_id}_decoder_attentions.pt\")\n",
    "            else:\n",
    "                print(f\"Skipping {uniprot_id} attentions matrices and logits\")\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time elapsed: {end_time-start_time}\")\n",
    "print(f\"Time per protein: {(end_time-start_time)/(end-start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'end_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4e8b494c3439>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# seconds per iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time per iteration: {(end_time-start_time)/total_iterations}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m## as it is without saving logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Time per iteration: 0.6380714930333162\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'end_time' is not defined"
     ]
    }
   ],
   "source": [
    "# seconds per iteration\n",
    "print(f\"Time per iteration: {(end_time-start_time)/total_iterations}\")\n",
    "## as it is without saving logits\n",
    "# Time per iteration: 0.6380714930333162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing A0A087WV00, protnum 1\n",
      "#### Mask size: 1 ####\n",
      "105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1057/1057 [00:00<00:00, 38077.31it/s]\n",
      "  0%|          | 0/105 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.69 GiB total capacity; 21.74 GiB already allocated; 163.81 MiB free; 21.98 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e147eed75df1>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfullmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_true_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0mprotein_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39bioembed/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39bioembed/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m             \u001b[0;31m# Convert encoder inputs in embeddings if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39bioembed/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39bioembed/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 )\n\u001b[1;32m   1108\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1110\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39bioembed/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39bioembed/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    690\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39bioembed/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39bioembed/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    594\u001b[0m     ):\n\u001b[1;32m    595\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39bioembed/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39bioembed/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.69 GiB total capacity; 21.74 GiB already allocated; 163.81 MiB free; 21.98 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "### The idea of this code was to process batches of masked sequences, with the hopes of being faster\n",
    "### But memory availability is a problem, I always run out of memory with long sequences, when batch size is larger than 5-6 \n",
    "\n",
    "import time\n",
    "\n",
    "start = 1\n",
    "end = 3\n",
    "start_time = time.time()\n",
    "counter = 1\n",
    "batch_size = 10\n",
    "total_iterations = 0\n",
    "\n",
    "for record in SeqIO.parse(fastafile, \"fasta\"):\n",
    "    #sequences.append(record)\n",
    "    if counter >= start and counter < end:\n",
    "        uniprot_id = record.id\n",
    "        aa_sequence = str(record.seq)\n",
    "        \n",
    "        pred_dict = dict()\n",
    "        mask_sizes = [1]\n",
    "        print(f\" Processing {uniprot_id}, protnum {counter}\")\n",
    "        if uniprot_id not in pred_dict:\n",
    "            pred_dict[uniprot_id] = dict()\n",
    "        \n",
    "        target_seq = aa_sequence\n",
    "        input_seq = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", target_seq)))]\n",
    "        true_input = tokenizer(input_seq)\n",
    "        true_tok = torch.tensor(true_input['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(true_input['attention_mask']).to(device)\n",
    "        if not os.path.exists(f\"{outdir}/{uniprot_id}.json\"):\n",
    "            for mask_size in mask_sizes:\n",
    "                print(f\"#### Mask size: {mask_size} ####\")\n",
    "                    \n",
    "                torch.cuda.empty_cache()\n",
    "                # loss_sequence = list()\n",
    "                match_sequence = list()\n",
    "                logits_sequence = list()\n",
    "                all_input_ids = list()\n",
    "                all_masked_seqs = list()\n",
    "\n",
    "                n_batches = len(target_seq)//batch_size\n",
    "                print(n_batches)\n",
    "\n",
    "\n",
    "                for i in tqdm(range(len(target_seq)-mask_size+1)):\n",
    "                    masked_seq = [sequence_masker(input_seq[0], i, i+mask_size)]\n",
    "                    all_masked_seqs.append(masked_seq[0])\n",
    "                    #all_input_ids.append(input_ids)\n",
    "\n",
    "                for i in tqdm(range(n_batches)):\n",
    "                    batch_end = min((i+1)*batch_size, len(all_masked_seqs))\n",
    "                    current_batch_size = batch_end - i*batch_size\n",
    "                    batch_ids = tokenizer.batch_encode_plus(all_masked_seqs[i*batch_size:batch_end], padding=\"longest\")\n",
    "                    tokenized_sequences = torch.tensor(batch_ids[\"input_ids\"]).to(device)\n",
    "                    tokenized_true_sequences = true_tok.repeat(current_batch_size,1)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        try:\n",
    "                            del emb\n",
    "                            torch.cuda.empty_cache()\n",
    "                        except:\n",
    "                            pass\n",
    "                        emb = fullmodel(input_ids=tokenized_sequences, labels=tokenized_true_sequences, output_attentions=output_attentions)\n",
    "                    protein_loss = emb.loss.cpu().detach()\n",
    "                    \n",
    "                    cpulogits = emb.logits.cpu().detach().squeeze()[:,:-1,:]\n",
    "                    torch.cuda.empty_cache()\n",
    "                    ## fastpred = tokenizer.decode(torch.tensor(cpulogits[:,:-1,:].numpy().argmax(-1)[0]), skip_special_tokens=False).replace(\"<\",\" <\").replace(\">\",\"> \")\n",
    "                    for j in range(cpulogits.shape[0]):\n",
    "                        fastpred = tokenizer.decode(torch.argmax(cpulogits[j], dim=1), skip_special_tokens=False).replace(\"<\",\" <\").replace(\">\",\"> \")\n",
    "\n",
    "                        if input_seq[0] == fastpred:\n",
    "                            match_sequence.append(True)\n",
    "                        else:\n",
    "                            pred_arr = fastpred.split()\n",
    "                            seq_arr  = input_seq[0].split()\n",
    "                            if len(pred_arr) == len(seq_arr):\n",
    "                                local_match_sequence = list()\n",
    "                                for j in range(len(pred_arr)):\n",
    "                                    if pred_arr[j] != seq_arr[j]:\n",
    "                                        local_match_sequence.append((j,pred_arr[j], seq_arr[j]))\n",
    "                                match_sequence.append(local_match_sequence)\n",
    "                            else:\n",
    "                                print(f\"{i} - Mismatch length error\")\n",
    "                                match_sequence.append(False)\n",
    "\n",
    "                    logits_sequence += cpulogits.numpy().tolist()\n",
    "\n",
    "                    total_iterations += 1                        \n",
    "                        \n",
    "                pred_dict[uniprot_id][f\"aamask_{mask_size}\"] = dict()\n",
    "                pred_dict[uniprot_id][f\"aamask_{mask_size}\"][\"match\"] = match_sequence\n",
    "                # pred_dict[uniprot_id][f\"aamask_{mask_size}\"][\"loss\"] = loss_sequence\n",
    "                ### This takes too much time and space, we will save the logits somewhere else\n",
    "                # pred_dict[uniprot_id][f\"aamask_{mask_size}\"][\"logits\"] = logits_sequence\n",
    "                np.save(f\"{outdir}/logits/{uniprot_id}_logits_sequence.npy\",  np.array(logits_sequence, dtype=object), allow_pickle=True)\n",
    "            with open(f\"{outdir}/{uniprot_id}.json\", 'w') as outfmt:\n",
    "                json.dump(pred_dict, outfmt)\n",
    "        else:\n",
    "            print(f\"Skipping {uniprot_id} masks\")\n",
    "    counter += 1\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"Time elapsed: {end_time-start_time}\")\n",
    "print(f\"Time per protein: {(end_time-start_time)/(end-start)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
