{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.patches import Patch\n",
    "import mpl_stylesheet\n",
    "import re\n",
    "import gc\n",
    "# mpl_stylesheet.banskt_presentation(fontfamily = 'mono', fontsize = 20, colors = 'banskt', dpi = 300)\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "fullmodel = T5ForConditionalGeneration.from_pretrained(\"../models/prottrans_t5_xl_u50/\").to(device)\n",
    "\n",
    "#fullmodel.full() if str(device)=='cpu' else fullmodel.half()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('../models/prottrans_t5_xl_u50', do_lower_case=False, legacy=False)\n",
    "\n",
    "def sequence_masker(seq, i, j, same_extra_token=False):\n",
    "    masked_sequence_list = seq.split()\n",
    "    token_num = 0\n",
    "    if j<=i:\n",
    "        print(f\"index j={j} must be greater than i={i}\")\n",
    "        raise\n",
    "    for x in range(i, j):\n",
    "        if j > len(seq):\n",
    "            break\n",
    "        masked_sequence_list[x] = f\"<extra_id_{token_num}>\"\n",
    "        if not same_extra_token:\n",
    "            token_num += 1\n",
    "    return \" \".join(masked_sequence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one dataset\n",
    "\n",
    "import json\n",
    "\n",
    "disprot_file = \"../AF2_testset.json\"\n",
    "with open(disprot_file) as infmt:\n",
    "    disprot_dict = json.load(infmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"_from_model_config\": true,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"transformers_version\": \"4.31.0\"\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fullmodel.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P37840\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 140/140 [00:09<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 136/136 [00:08<00:00, 16.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P04637\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 393/393 [00:35<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 389/389 [00:35<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P02686\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 304/304 [00:21<00:00, 14.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 300/300 [00:20<00:00, 14.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P07305\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 194/194 [00:12<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 190/190 [00:12<00:00, 15.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O00488\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 134/134 [00:08<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 130/130 [00:08<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q9NYB9\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 513/513 [01:11<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 509/509 [01:10<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P06401\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 933/933 [05:52<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 929/929 [05:52<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q16186\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 407/407 [00:36<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 403/403 [00:37<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S6B291\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 466/466 [00:52<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 462/462 [00:52<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P23441\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 372/372 [00:29<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 368/368 [00:29<00:00, 12.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "protids = list(disprot_dict.keys())\n",
    "loss_dict = dict()\n",
    "predmatch_dict = dict()\n",
    "mask_sizes = [1, 5]\n",
    "for test_prot in protids:\n",
    "    print(test_prot)\n",
    "    if test_prot not in predmatch_dict:\n",
    "        predmatch_dict[test_prot] = dict()\n",
    "    if test_prot not in loss_dict:\n",
    "        loss_dict[test_prot] = dict()\n",
    "    \n",
    "    target_seq = disprot_dict[test_prot]['seq']\n",
    "    input_seq = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", target_seq)))]\n",
    "    true_input = tokenizer(input_seq)\n",
    "    true_tok = torch.tensor(true_input['input_ids']).to(device)\n",
    "\n",
    "    for mask_size in mask_sizes:\n",
    "        print(f\"#### Mask size: {mask_size} ####\")\n",
    "               \n",
    "        loss_sequence = list()\n",
    "        match_sequence = list()\n",
    "        for i in tqdm(range(len(target_seq)-mask_size+1)):\n",
    "\n",
    "            masked_seq = [sequence_masker(input_seq[0], i, i+mask_size)]\n",
    "            tmp = tokenizer(masked_seq)\n",
    "            input_ids = torch.tensor(tmp['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(tmp['attention_mask']).to(device)\n",
    "            with torch.no_grad():\n",
    "                emb  = fullmodel(input_ids=input_ids, labels=true_tok)\n",
    "                loss = emb.loss.cpu()\n",
    "                loss_sequence.append(loss.item())\n",
    "                cpulogits = emb.logits.cpu()\n",
    "                fastpred = tokenizer.decode(torch.tensor(cpulogits[:,:-1,:].numpy().argmax(-1)[0]), skip_special_tokens=False).replace(\"<\",\" <\").replace(\">\",\"> \")\n",
    "    #             outputs = fullmodel.generate(input_ids=input_ids, max_length=input_ids.shape[1]+10)\n",
    "    #             prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    #             if fastpred == prediction:\n",
    "    #                 print(\"OK \", end=\" \")\n",
    "                if input_seq[0] == fastpred:\n",
    "                    match_sequence.append(True)\n",
    "                else:\n",
    "                    pred_arr = fastpred.split()\n",
    "                    seq_arr  = input_seq[0].split()\n",
    "                    if len(pred_arr) == len(seq_arr):\n",
    "                        local_match_sequence = list()\n",
    "                        for j in range(len(pred_arr)):\n",
    "                            if pred_arr[j] != seq_arr[j]:\n",
    "                                local_match_sequence.append((j,pred_arr[j], seq_arr[j]))\n",
    "                    else:\n",
    "                        print(\"Mismatch length error\")\n",
    "                        raise\n",
    "                    match_sequence.append(local_match_sequence)\n",
    "        predmatch_dict[test_prot][f\"aa{mask_size}_match\"] = match_sequence\n",
    "        \n",
    "        loss_dict[test_prot][f\"aa{mask_size}_loss\"] = loss_sequence\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(np.array(cpulogits[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"disprot_multi_loss.json\", 'w') as outfmt:\n",
    "    json.dump(loss_dict, outfmt)\n",
    "with open(\"disprot_multi_mismatch.json\", 'w') as outfmt:\n",
    "    json.dump(predmatch_dict, outfmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.scatter(np.arange(len(target_seq)), loss_sequence)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monomer_file = \"monomer_datadict.json\"\n",
    "with open(monomer_file) as infmt:\n",
    "    monomer_dict = json.load(infmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1AE9A\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 179/179 [00:11<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 175/175 [00:11<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1AH7A\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 245/245 [00:16<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 241/241 [00:16<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1AHOA\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 64/64 [00:03<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 60/60 [00:03<00:00, 16.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1AOCA\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 175/175 [00:11<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 171/171 [00:10<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1AOLA\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 228/228 [00:14<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 224/224 [00:14<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1AQZA\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 149/149 [00:09<00:00, 15.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 145/145 [00:09<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ATGA\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 231/231 [00:15<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 227/227 [00:14<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ATZA\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 189/189 [00:12<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 185/185 [00:11<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1AYOB\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 130/130 [00:08<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 126/126 [00:07<00:00, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1AZOA\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 232/232 [00:15<00:00, 15.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 228/228 [00:15<00:00, 14.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1B9WA\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 95/95 [00:05<00:00, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Mask size: 5 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 91/91 [00:05<00:00, 16.31it/s]\n"
     ]
    }
   ],
   "source": [
    "protids = list(monomer_dict.keys())\n",
    "loss_dict = dict()\n",
    "predmatch_dict = dict()\n",
    "mask_sizes = [1, 5]\n",
    "for test_prot in protids:\n",
    "    print(test_prot)\n",
    "    if test_prot not in predmatch_dict:\n",
    "        predmatch_dict[test_prot] = dict()\n",
    "    if test_prot not in loss_dict:\n",
    "        loss_dict[test_prot] = dict()\n",
    "    \n",
    "    target_seq = monomer_dict[test_prot]['seq']\n",
    "    input_seq = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", target_seq)))]\n",
    "    true_input = tokenizer(input_seq)\n",
    "    true_tok = torch.tensor(true_input['input_ids']).to(device)\n",
    "\n",
    "    for mask_size in mask_sizes:\n",
    "        print(f\"#### Mask size: {mask_size} ####\")\n",
    "               \n",
    "        loss_sequence = list()\n",
    "        match_sequence = list()\n",
    "        for i in tqdm(range(len(target_seq)-mask_size+1)):\n",
    "\n",
    "            masked_seq = [sequence_masker(input_seq[0], i, i+mask_size)]\n",
    "            tmp = tokenizer(masked_seq)\n",
    "            input_ids = torch.tensor(tmp['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(tmp['attention_mask']).to(device)\n",
    "            with torch.no_grad():\n",
    "                emb  = fullmodel(input_ids=input_ids, labels=true_tok)\n",
    "                loss = emb.loss.cpu()\n",
    "                loss_sequence.append(loss.item())\n",
    "                cpulogits = emb.logits.cpu()\n",
    "                fastpred = tokenizer.decode(torch.tensor(cpulogits[:,:-1,:].numpy().argmax(-1)[0]), skip_special_tokens=False).replace(\"<\",\" <\").replace(\">\",\"> \")\n",
    "                if input_seq[0] == fastpred:\n",
    "                    match_sequence.append(True)\n",
    "                else:\n",
    "                    pred_arr = fastpred.split()\n",
    "                    seq_arr  = input_seq[0].split()\n",
    "                    if len(pred_arr) == len(seq_arr):\n",
    "                        local_match_sequence = list()\n",
    "                        for j in range(len(pred_arr)):\n",
    "                            if pred_arr[j] != seq_arr[j]:\n",
    "                                local_match_sequence.append((j,pred_arr[j], seq_arr[j]))\n",
    "                    else:\n",
    "                        print(\"Mismatch length error\")\n",
    "                        raise\n",
    "                    match_sequence.append(local_match_sequence)\n",
    "        predmatch_dict[test_prot][f\"aa{mask_size}_match\"] = match_sequence\n",
    "        \n",
    "        loss_dict[test_prot][f\"aa{mask_size}_loss\"] = loss_sequence\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    \n",
    "with open(\"monomer_multi_loss.json\", 'w') as outfmt:\n",
    "    json.dump(loss_dict, outfmt)\n",
    "with open(\"monomer_multi_mismatch.json\", 'w') as outfmt:\n",
    "    json.dump(predmatch_dict, outfmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
