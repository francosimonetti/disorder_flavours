{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "\t- Output attentions: True\n",
      "\t- Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "import esm\n",
    "import torch\n",
    "import json\n",
    "\n",
    "fastafile = \"/biodata/franco/datasets/disprot/disprot_OK_fullset_2023_12.fasta\"\n",
    "\n",
    "start = 0\n",
    "end = 1\n",
    "outdir = \"ESM2_dev_output\"\n",
    "output_attentions = True\n",
    "\n",
    "\n",
    "def sequence_masker(seq, i, j):\n",
    "    masked_sequence_list = seq.split()\n",
    "    if j<=i:\n",
    "        print(f\"index j={j} must be greater than i={i}\")\n",
    "        raise\n",
    "    for x in range(i, j):\n",
    "        if j > len(seq):\n",
    "            break\n",
    "        masked_sequence_list[x] = f\"<mask>\"\n",
    "    return \" \".join(masked_sequence_list)\n",
    "\n",
    "\n",
    "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"Parameters:\")\n",
    "print(f\"\\t- Output attentions: {output_attentions}\")\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "if not os.path.exists(outdir+\"/logits\"):\n",
    "    os.makedirs(outdir+\"/logits\")\n",
    "if output_attentions:\n",
    "    if not os.path.exists(outdir+\"/attentions\"):\n",
    "        os.makedirs(outdir+\"/attentions\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\t- Device: {device}\")\n",
    "# Load model and tokenizer\n",
    "model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    model = model.eval().cuda()  # disables dropout for deterministic results\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    model = model.eval()\n",
    "\n",
    "# Define the loss function obj\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Read fasta sequences\n",
    "counter = 0\n",
    "batch_converter = alphabet.get_batch_converter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ESM2',\n",
       " 'Namespace',\n",
       " 'Path',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_download_model_and_regression_data',\n",
       " '_has_regression_weights',\n",
       " '_load_model_and_alphabet_core_v1',\n",
       " '_load_model_and_alphabet_core_v2',\n",
       " 'esm',\n",
       " 'esm1_t12_85M_UR50S',\n",
       " 'esm1_t34_670M_UR100',\n",
       " 'esm1_t34_670M_UR50D',\n",
       " 'esm1_t34_670M_UR50S',\n",
       " 'esm1_t6_43M_UR50S',\n",
       " 'esm1b_t33_650M_UR50S',\n",
       " 'esm1v_t33_650M_UR90S',\n",
       " 'esm1v_t33_650M_UR90S_1',\n",
       " 'esm1v_t33_650M_UR90S_2',\n",
       " 'esm1v_t33_650M_UR90S_3',\n",
       " 'esm1v_t33_650M_UR90S_4',\n",
       " 'esm1v_t33_650M_UR90S_5',\n",
       " 'esm2_t12_35M_UR50D',\n",
       " 'esm2_t30_150M_UR50D',\n",
       " 'esm2_t33_650M_UR50D',\n",
       " 'esm2_t36_3B_UR50D',\n",
       " 'esm2_t48_15B_UR50D',\n",
       " 'esm2_t6_8M_UR50D',\n",
       " 'esm_if1_gvp4_t16_142M_UR50',\n",
       " 'esm_msa1_t12_100M_UR50S',\n",
       " 'esm_msa1b_t12_100M_UR50S',\n",
       " 'esmfold_structure_module_only_150M',\n",
       " 'esmfold_structure_module_only_150M_270K',\n",
       " 'esmfold_structure_module_only_15B',\n",
       " 'esmfold_structure_module_only_35M',\n",
       " 'esmfold_structure_module_only_35M_270K',\n",
       " 'esmfold_structure_module_only_3B',\n",
       " 'esmfold_structure_module_only_3B_270K',\n",
       " 'esmfold_structure_module_only_650M',\n",
       " 'esmfold_structure_module_only_650M_270K',\n",
       " 'esmfold_structure_module_only_8M',\n",
       " 'esmfold_structure_module_only_8M_270K',\n",
       " 'esmfold_v0',\n",
       " 'esmfold_v1',\n",
       " 'has_emb_layer_norm_before',\n",
       " 'load_hub_workaround',\n",
       " 'load_model_and_alphabet',\n",
       " 'load_model_and_alphabet_core',\n",
       " 'load_model_and_alphabet_hub',\n",
       " 'load_model_and_alphabet_local',\n",
       " 'load_regression_hub',\n",
       " 're',\n",
       " 'torch',\n",
       " 'urllib',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(esm.pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "lengths = []\n",
    "for record in SeqIO.parse(fastafile, \"fasta\"):\n",
    "    sequences.append(str(record.seq))\n",
    "    lengths.append(len(str(record.seq)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "ix = lengths.index(np.max(lengths))\n",
    "ix = lengths.index(300)\n",
    "longest_seq = sequences[ix]\n",
    "start = ix\n",
    "end = start+1\n",
    "print(ix)\n",
    "print(len(longest_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing Q8GY88, protnum 1521, len: 300\n",
      "#### Mask size: 1 ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [02:07<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "if output_attentions:\n",
    "    need_head_weights = True\n",
    "else:\n",
    "    need_head_weights = False\n",
    "for record in SeqIO.parse(fastafile, \"fasta\"):\n",
    "    #sequences.append(record)\n",
    "    if counter >= start and counter < end:\n",
    "        uniprot_id = record.id\n",
    "        aa_sequence = str(record.seq)\n",
    "\n",
    "        pred_dict = dict()\n",
    "        mask_sizes = [1]\n",
    "        print(f\" Processing {uniprot_id}, protnum {counter}, len: {len(aa_sequence)}\")\n",
    "        if uniprot_id not in pred_dict:\n",
    "            pred_dict[uniprot_id] = dict()\n",
    "        \n",
    "        target_seq = aa_sequence\n",
    "        input_seq = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", target_seq)))]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter([(uniprot_id, input_seq[0])])\n",
    "        if not os.path.exists(f\"{outdir}/{uniprot_id}.json\"):\n",
    "            for mask_size in mask_sizes:\n",
    "                print(f\"#### Mask size: {mask_size} ####\")\n",
    "                    \n",
    "                loss_sequence = list()\n",
    "                match_sequence = list()\n",
    "                logits_sequence = list()\n",
    "                meanatt_sequence = list()\n",
    "                maxatt_sequence = list()\n",
    "                for i in tqdm(range(len(target_seq)-mask_size+1)):\n",
    "\n",
    "                    masked_seq = sequence_masker(input_seq[0], i, i+mask_size)\n",
    "                    mbatch_labels, mbatch_strs, mbatch_tokens = batch_converter([(uniprot_id, masked_seq)])\n",
    "                    with torch.no_grad():\n",
    "                        results = model(mbatch_tokens.to(device), repr_layers=[36], return_contacts=False, need_head_weights=need_head_weights)\n",
    "                    cpulogits = results['logits'][0].cpu()\n",
    "                    loss_val = float(loss(cpulogits[1:-1,], mbatch_tokens[0][1:-1]).numpy())  ## recently corrected to discard cls and eos tokens\n",
    "                    loss_sequence.append(loss_val)\n",
    "                    logits_sequence.append(cpulogits[1:-1,].numpy().tolist())\n",
    "                    #fastpred = tokenizer.decode(torch.tensor(cpulogits[:,:-1,:].numpy().argmax(-1)[0]), skip_special_tokens=False).replace(\"<\",\" <\").replace(\">\",\"> \")\n",
    "                    fastpred = \" \".join([alphabet.get_tok(t) for t in results['logits'][0].cpu().numpy().argmax(-1)][1:-1]) ## delete first and last tokens\n",
    "                    if input_seq[0] == fastpred:\n",
    "                        match_sequence.append(True)\n",
    "                    else:\n",
    "                        pred_arr = fastpred.split()\n",
    "                        seq_arr  = input_seq[0].split()\n",
    "                        if len(pred_arr) == len(seq_arr):\n",
    "                            local_match_sequence = list()\n",
    "                            for j in range(len(pred_arr)):\n",
    "                                if pred_arr[j] != seq_arr[j]:\n",
    "                                    local_match_sequence.append((j,pred_arr[j], seq_arr[j]))\n",
    "                            match_sequence.append(local_match_sequence)\n",
    "                        else:\n",
    "                            print(f\"{i} - Mismatch length error\")\n",
    "                            match_sequence.append(False)\n",
    "                            loss_sequence\n",
    "                    if output_attentions:\n",
    "                        att_cpu = results['attentions'].squeeze().cpu()\n",
    "                        fullmax = torch.amax(att_cpu, dim=(0,1))\n",
    "                        fullmean = torch.mean(att_cpu, dim=(0,1))\n",
    "                        meanatt_sequence.append(fullmean)\n",
    "                        maxatt_sequence.append(fullmean)\n",
    "                pred_dict[uniprot_id][f\"aamask_{mask_size}\"] = dict()\n",
    "                pred_dict[uniprot_id][f\"aamask_{mask_size}\"][\"match\"] = match_sequence\n",
    "                pred_dict[uniprot_id][f\"aamask_{mask_size}\"][\"loss\"] = loss_sequence\n",
    "                ### This takes too much time and space, we will save the logits somewhere else\n",
    "                #pred_dict[uniprot_id][f\"aamask_{mask_size}\"][\"logits\"] = logits_sequence\n",
    "                np.save(f\"{outdir}/logits/{uniprot_id}_logits_sequence.npy\",  np.array(logits_sequence, dtype=object), allow_pickle=True)\n",
    "                if output_attentions:\n",
    "                    np.save(f\"{outdir}/attentions/{uniprot_id}_max_attentions_sequence.npy\",  np.array(meanatt_sequence, dtype=object), allow_pickle=True)\n",
    "                    np.save(f\"{outdir}/attentions/{uniprot_id}_mean_attentions_sequence.npy\",  np.array(maxatt_sequence, dtype=object), allow_pickle=True)\n",
    "            with open(f\"{outdir}/{uniprot_id}.json\", 'w') as outfmt:\n",
    "                json.dump(pred_dict, outfmt)\n",
    "        else:\n",
    "            print(f\"Skipping {uniprot_id} masks\")\n",
    "        if not os.path.exists(f\"{outdir}/logits/{uniprot_id}_logits.pt\"):\n",
    "            ## Output the complete attention matrices with a full pass, no mask\n",
    "            with torch.no_grad():\n",
    "                #emb = fullmodel(input_ids=true_tok, labels=true_tok, output_attentions=output_attentions, attention_mask=attention_mask, decoder_attention_mask=attention_mask)\n",
    "                if device.type == 'cuda':\n",
    "                    # if len(aa_sequence) > 600:\n",
    "                    #     results = model(batch_tokens.to(device), repr_layers=[36], return_contacts=False)\n",
    "                    # else:\n",
    "                    results = model(batch_tokens.to(device), repr_layers=[36], return_contacts=False, need_head_weights=need_head_weights)\n",
    "                    # torch.save(results['contacts'][0].cpu(), f\"{outdir}/logits/{uniprot_id}_contacts.pt\")\n",
    "                else:\n",
    "                    results = model(batch_tokens.to(device), repr_layers=[36], return_contacts=True, need_head_weights=need_head_weights)\n",
    "                    torch.save(results['contacts'][0].cpu(), f\"{outdir}/logits/{uniprot_id}_contacts.pt\")\n",
    "            cpulogits = results['logits'][0].cpu()[1:-1,]\n",
    "            torch.save(cpulogits, f\"{outdir}/logits/{uniprot_id}_logits.pt\")\n",
    "            if output_attentions:\n",
    "                att_cpu = results['attentions'].squeeze().cpu()\n",
    "                fullmax = torch.amax(att_cpu, dim=(0,1))\n",
    "                fullmean = torch.mean(att_cpu, dim=(0,1))\n",
    "                torch.save(fullmax, f\"{outdir}/attentions/{uniprot_id}_original_max_attentions.pt\")\n",
    "                torch.save(fullmean, f\"{outdir}/attentions/{uniprot_id}_original_mean_attentions.pt\")\n",
    "        else:\n",
    "            print(f\"Skipping {uniprot_id} attentions matrices and logits\")\n",
    "    counter += 1\n",
    "    if device == 'cuda:0':\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2164"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([True for l in lengths if l<700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529,\n",
       " 170,\n",
       " 107,\n",
       " 105,\n",
       " 318,\n",
       " 256,\n",
       " 122,\n",
       " 295,\n",
       " 165,\n",
       " 76,\n",
       " 164,\n",
       " 316,\n",
       " 198,\n",
       " 827,\n",
       " 691,\n",
       " 328,\n",
       " 98,\n",
       " 1018,\n",
       " 495,\n",
       " 97,\n",
       " 118,\n",
       " 211,\n",
       " 777,\n",
       " 293,\n",
       " 506,\n",
       " 638,\n",
       " 424,\n",
       " 677,\n",
       " 107,\n",
       " 70,\n",
       " 239,\n",
       " 190,\n",
       " 169,\n",
       " 206,\n",
       " 423,\n",
       " 350,\n",
       " 416,\n",
       " 756,\n",
       " 62,\n",
       " 336,\n",
       " 616,\n",
       " 462,\n",
       " 205,\n",
       " 279,\n",
       " 147,\n",
       " 206,\n",
       " 116,\n",
       " 140,\n",
       " 372,\n",
       " 313,\n",
       " 595,\n",
       " 765,\n",
       " 380,\n",
       " 530,\n",
       " 952,\n",
       " 281,\n",
       " 160,\n",
       " 551,\n",
       " 393,\n",
       " 490,\n",
       " 663,\n",
       " 315,\n",
       " 419,\n",
       " 267,\n",
       " 521,\n",
       " 498,\n",
       " 262,\n",
       " 194,\n",
       " 423,\n",
       " 443,\n",
       " 500,\n",
       " 197,\n",
       " 286,\n",
       " 376,\n",
       " 421,\n",
       " 247,\n",
       " 440,\n",
       " 155,\n",
       " 117,\n",
       " 49,\n",
       " 449,\n",
       " 310,\n",
       " 771,\n",
       " 458,\n",
       " 651,\n",
       " 549,\n",
       " 72,\n",
       " 395,\n",
       " 507,\n",
       " 632,\n",
       " 833,\n",
       " 219,\n",
       " 65,\n",
       " 177,\n",
       " 85,\n",
       " 209,\n",
       " 276,\n",
       " 92,\n",
       " 512,\n",
       " 549,\n",
       " 336,\n",
       " 189,\n",
       " 288,\n",
       " 363,\n",
       " 685,\n",
       " 884,\n",
       " 328,\n",
       " 88,\n",
       " 265,\n",
       " 106,\n",
       " 210,\n",
       " 261,\n",
       " 445,\n",
       " 648,\n",
       " 331,\n",
       " 619,\n",
       " 720,\n",
       " 961,\n",
       " 68,\n",
       " 222,\n",
       " 362,\n",
       " 427,\n",
       " 143,\n",
       " 254,\n",
       " 332,\n",
       " 206,\n",
       " 599,\n",
       " 190,\n",
       " 165,\n",
       " 90,\n",
       " 708,\n",
       " 180,\n",
       " 164,\n",
       " 108,\n",
       " 65,\n",
       " 314,\n",
       " 200,\n",
       " 126,\n",
       " 346,\n",
       " 720,\n",
       " 433,\n",
       " 915,\n",
       " 229,\n",
       " 408,\n",
       " 540,\n",
       " 743,\n",
       " 741,\n",
       " 304,\n",
       " 639,\n",
       " 737,\n",
       " 726,\n",
       " 84,\n",
       " 273,\n",
       " 380,\n",
       " 453,\n",
       " 161,\n",
       " 258,\n",
       " 575,\n",
       " 268,\n",
       " 309,\n",
       " 373,\n",
       " 196,\n",
       " 439,\n",
       " 826,\n",
       " 254,\n",
       " 350,\n",
       " 69,\n",
       " 382,\n",
       " 163,\n",
       " 575,\n",
       " 848,\n",
       " 341,\n",
       " 213,\n",
       " 73,\n",
       " 256,\n",
       " 114,\n",
       " 239,\n",
       " 233,\n",
       " 111,\n",
       " 159,\n",
       " 481,\n",
       " 199,\n",
       " 323,\n",
       " 367,\n",
       " 437,\n",
       " 287,\n",
       " 639,\n",
       " 458,\n",
       " 636,\n",
       " 402,\n",
       " 663,\n",
       " 313,\n",
       " 434,\n",
       " 372,\n",
       " 166,\n",
       " 338,\n",
       " 581,\n",
       " 224,\n",
       " 317,\n",
       " 417,\n",
       " 491,\n",
       " 169,\n",
       " 152,\n",
       " 354,\n",
       " 137,\n",
       " 781,\n",
       " 582,\n",
       " 149,\n",
       " 953,\n",
       " 649,\n",
       " 87,\n",
       " 321,\n",
       " 151,\n",
       " 910,\n",
       " 316,\n",
       " 354,\n",
       " 270,\n",
       " 44,\n",
       " 160,\n",
       " 107,\n",
       " 522,\n",
       " 184,\n",
       " 106,\n",
       " 232,\n",
       " 785,\n",
       " 348,\n",
       " 111,\n",
       " 848,\n",
       " 102,\n",
       " 215,\n",
       " 420,\n",
       " 116,\n",
       " 418,\n",
       " 790,\n",
       " 156,\n",
       " 800,\n",
       " 662,\n",
       " 330,\n",
       " 687,\n",
       " 503,\n",
       " 365,\n",
       " 654,\n",
       " 97,\n",
       " 151,\n",
       " 156,\n",
       " 854,\n",
       " 202,\n",
       " 98,\n",
       " 258,\n",
       " 116,\n",
       " 887,\n",
       " 603,\n",
       " 419,\n",
       " 93,\n",
       " 176,\n",
       " 806,\n",
       " 445,\n",
       " 418,\n",
       " 194,\n",
       " 175,\n",
       " 225,\n",
       " 241,\n",
       " 233,\n",
       " 435,\n",
       " 152,\n",
       " 650,\n",
       " 387,\n",
       " 390,\n",
       " 991,\n",
       " 552,\n",
       " 253,\n",
       " 317,\n",
       " 168,\n",
       " 639,\n",
       " 593,\n",
       " 681,\n",
       " 58,\n",
       " 162,\n",
       " 346,\n",
       " 640,\n",
       " 399,\n",
       " 345,\n",
       " 360,\n",
       " 538,\n",
       " 893,\n",
       " 920,\n",
       " 430,\n",
       " 171,\n",
       " 207,\n",
       " 182,\n",
       " 86,\n",
       " 82,\n",
       " 133,\n",
       " 101,\n",
       " 253,\n",
       " 451,\n",
       " 202,\n",
       " 112,\n",
       " 168,\n",
       " 115,\n",
       " 978,\n",
       " 131,\n",
       " 478,\n",
       " 486,\n",
       " 345,\n",
       " 590,\n",
       " 63,\n",
       " 120,\n",
       " 129,\n",
       " 134,\n",
       " 108,\n",
       " 134,\n",
       " 216,\n",
       " 848,\n",
       " 134,\n",
       " 379,\n",
       " 398,\n",
       " 508,\n",
       " 219,\n",
       " 204,\n",
       " 170,\n",
       " 170,\n",
       " 112,\n",
       " 144,\n",
       " 212,\n",
       " 361,\n",
       " 505,\n",
       " 50,\n",
       " 206,\n",
       " 193,\n",
       " 613,\n",
       " 243,\n",
       " 62,\n",
       " 201,\n",
       " 663,\n",
       " 260,\n",
       " 210,\n",
       " 536,\n",
       " 646,\n",
       " 378,\n",
       " 248,\n",
       " 710,\n",
       " 70,\n",
       " 548,\n",
       " 432,\n",
       " 705,\n",
       " 116,\n",
       " 389,\n",
       " 169,\n",
       " 327,\n",
       " 139,\n",
       " 386,\n",
       " 524,\n",
       " 127,\n",
       " 284,\n",
       " 525,\n",
       " 97,\n",
       " 185,\n",
       " 358,\n",
       " 154,\n",
       " 725,\n",
       " 677,\n",
       " 358,\n",
       " 265,\n",
       " 193,\n",
       " 385,\n",
       " 105,\n",
       " 462,\n",
       " 171,\n",
       " 173,\n",
       " 185,\n",
       " 189,\n",
       " 183,\n",
       " 383,\n",
       " 237,\n",
       " 151,\n",
       " 864,\n",
       " 234,\n",
       " 94,\n",
       " 602,\n",
       " 215,\n",
       " 186,\n",
       " 189,\n",
       " 228,\n",
       " 94,\n",
       " 532,\n",
       " 532,\n",
       " 709,\n",
       " 707,\n",
       " 504,\n",
       " 165,\n",
       " 109,\n",
       " 529,\n",
       " 740,\n",
       " 739,\n",
       " 126,\n",
       " 664,\n",
       " 235,\n",
       " 505,\n",
       " 410,\n",
       " 723,\n",
       " 178,\n",
       " 345,\n",
       " 533,\n",
       " 165,\n",
       " 346,\n",
       " 246,\n",
       " 100,\n",
       " 109,\n",
       " 855,\n",
       " 859,\n",
       " 66,\n",
       " 459,\n",
       " 151,\n",
       " 304,\n",
       " 445,\n",
       " 126,\n",
       " 189,\n",
       " 151,\n",
       " 94,\n",
       " 238,\n",
       " 110,\n",
       " 77,\n",
       " 143,\n",
       " 78,\n",
       " 134,\n",
       " 196,\n",
       " 397,\n",
       " 590,\n",
       " 162,\n",
       " 155,\n",
       " 449,\n",
       " 586,\n",
       " 137,\n",
       " 268,\n",
       " 70,\n",
       " 264,\n",
       " 162,\n",
       " 400,\n",
       " 184,\n",
       " 108,\n",
       " 111,\n",
       " 131,\n",
       " 115,\n",
       " 345,\n",
       " 44,\n",
       " 129,\n",
       " 167,\n",
       " 304,\n",
       " 96,\n",
       " 52,\n",
       " 144,\n",
       " 188,\n",
       " 132,\n",
       " 250,\n",
       " 130,\n",
       " 108,\n",
       " 140,\n",
       " 110,\n",
       " 206,\n",
       " 205,\n",
       " 70,\n",
       " 99,\n",
       " 137,\n",
       " 99,\n",
       " 83,\n",
       " 70,\n",
       " 80,\n",
       " 83,\n",
       " 283,\n",
       " 241,\n",
       " 407,\n",
       " 205,\n",
       " 419,\n",
       " 86,\n",
       " 109,\n",
       " 107,\n",
       " 179,\n",
       " 112,\n",
       " 155,\n",
       " 102,\n",
       " 337,\n",
       " 256,\n",
       " 147,\n",
       " 199,\n",
       " 165,\n",
       " 195,\n",
       " 151,\n",
       " 218,\n",
       " 201,\n",
       " 211,\n",
       " 149,\n",
       " 72,\n",
       " 279,\n",
       " 243,\n",
       " 565,\n",
       " 121,\n",
       " 489,\n",
       " 624,\n",
       " 477,\n",
       " 192,\n",
       " 64,\n",
       " 56,\n",
       " 124,\n",
       " 137,\n",
       " 147,\n",
       " 91,\n",
       " 168,\n",
       " 86,\n",
       " 77,\n",
       " 638,\n",
       " 82,\n",
       " 241,\n",
       " 497,\n",
       " 451,\n",
       " 112,\n",
       " 150,\n",
       " 807,\n",
       " 209,\n",
       " 250,\n",
       " 144,\n",
       " 213,\n",
       " 80,\n",
       " 84,\n",
       " 113,\n",
       " 142,\n",
       " 853,\n",
       " 193,\n",
       " 64,\n",
       " 146,\n",
       " 273,\n",
       " 71,\n",
       " 208,\n",
       " 192,\n",
       " 277,\n",
       " 213,\n",
       " 274,\n",
       " 72,\n",
       " 86,\n",
       " 227,\n",
       " 120,\n",
       " 186,\n",
       " 108,\n",
       " 76,\n",
       " 81,\n",
       " 485,\n",
       " 351,\n",
       " 568,\n",
       " 332,\n",
       " 145,\n",
       " 817,\n",
       " 241,\n",
       " 422,\n",
       " 238,\n",
       " 354,\n",
       " 842,\n",
       " 242,\n",
       " 89,\n",
       " 464,\n",
       " 123,\n",
       " 862,\n",
       " 851,\n",
       " 750,\n",
       " 337,\n",
       " 53,\n",
       " 120,\n",
       " 597,\n",
       " 391,\n",
       " 156,\n",
       " 204,\n",
       " 398,\n",
       " 79,\n",
       " 856,\n",
       " 80,\n",
       " 142,\n",
       " 167,\n",
       " 305,\n",
       " 69,\n",
       " 61,\n",
       " 63,\n",
       " 231,\n",
       " 316,\n",
       " 138,\n",
       " 369,\n",
       " 273,\n",
       " 85,\n",
       " 141,\n",
       " 253,\n",
       " 340,\n",
       " 104,\n",
       " 222,\n",
       " 76,\n",
       " 131,\n",
       " 839,\n",
       " 204,\n",
       " 185,\n",
       " 117,\n",
       " 72,\n",
       " 89,\n",
       " 188,\n",
       " 124,\n",
       " 510,\n",
       " 107,\n",
       " 241,\n",
       " 224,\n",
       " 173,\n",
       " 719,\n",
       " 359,\n",
       " 65,\n",
       " 226,\n",
       " 105,\n",
       " 206,\n",
       " 65,\n",
       " 594,\n",
       " 280,\n",
       " 125,\n",
       " 103,\n",
       " 190,\n",
       " 354,\n",
       " 408,\n",
       " 274,\n",
       " 102,\n",
       " 685,\n",
       " 466,\n",
       " 98,\n",
       " 103,\n",
       " 996,\n",
       " 139,\n",
       " 141,\n",
       " 237,\n",
       " 118,\n",
       " 1016,\n",
       " 724,\n",
       " 690,\n",
       " 264,\n",
       " 411,\n",
       " 101,\n",
       " 568,\n",
       " 422,\n",
       " 901,\n",
       " 86,\n",
       " 959,\n",
       " 823,\n",
       " 327,\n",
       " 476,\n",
       " 680,\n",
       " 610,\n",
       " 640,\n",
       " 630,\n",
       " 101,\n",
       " 89,\n",
       " 266,\n",
       " 88,\n",
       " 285,\n",
       " 88,\n",
       " 171,\n",
       " 151,\n",
       " 505,\n",
       " 350,\n",
       " 441,\n",
       " 169,\n",
       " 526,\n",
       " 672,\n",
       " 622,\n",
       " 414,\n",
       " 353,\n",
       " 624,\n",
       " 440,\n",
       " 724,\n",
       " 708,\n",
       " 377,\n",
       " 605,\n",
       " 499,\n",
       " 781,\n",
       " 410,\n",
       " 243,\n",
       " 400,\n",
       " 619,\n",
       " 457,\n",
       " 158,\n",
       " 651,\n",
       " 160,\n",
       " 655,\n",
       " 497,\n",
       " 153,\n",
       " 610,\n",
       " 312,\n",
       " 148,\n",
       " 580,\n",
       " 219,\n",
       " 56,\n",
       " 81,\n",
       " 65,\n",
       " 450,\n",
       " 289,\n",
       " 266,\n",
       " 750,\n",
       " 158,\n",
       " 398,\n",
       " 194,\n",
       " 130,\n",
       " 126,\n",
       " 97,\n",
       " 291,\n",
       " 504,\n",
       " 692,\n",
       " 279,\n",
       " 561,\n",
       " 911,\n",
       " 276,\n",
       " 477,\n",
       " 502,\n",
       " 503,\n",
       " 138,\n",
       " 513,\n",
       " 333,\n",
       " 283,\n",
       " 138,\n",
       " 699,\n",
       " 589,\n",
       " 95,\n",
       " 821,\n",
       " 128,\n",
       " 591,\n",
       " 326,\n",
       " 71,\n",
       " 171,\n",
       " 792,\n",
       " 328,\n",
       " 71,\n",
       " 377,\n",
       " 718,\n",
       " 350,\n",
       " 712,\n",
       " 868,\n",
       " 192,\n",
       " 872,\n",
       " 674,\n",
       " 129,\n",
       " 465,\n",
       " 755,\n",
       " 101,\n",
       " 322,\n",
       " 482,\n",
       " 662,\n",
       " 597,\n",
       " 815,\n",
       " 111,\n",
       " 875,\n",
       " 80,\n",
       " 243,\n",
       " 352,\n",
       " 123,\n",
       " 390,\n",
       " 114,\n",
       " 609,\n",
       " 732,\n",
       " 172,\n",
       " 115,\n",
       " 677,\n",
       " 532,\n",
       " 268,\n",
       " 614,\n",
       " 521,\n",
       " 770,\n",
       " 103,\n",
       " 425,\n",
       " 222,\n",
       " 139,\n",
       " 46,\n",
       " 388,\n",
       " 823,\n",
       " 104,\n",
       " 82,\n",
       " 467,\n",
       " 120,\n",
       " 1021,\n",
       " 72,\n",
       " 75,\n",
       " 97,\n",
       " 98,\n",
       " 115,\n",
       " 297,\n",
       " 808,\n",
       " 265,\n",
       " 372,\n",
       " 373,\n",
       " 477,\n",
       " 484,\n",
       " 547,\n",
       " 850,\n",
       " 324,\n",
       " 890,\n",
       " 467,\n",
       " 348,\n",
       " 638,\n",
       " 516,\n",
       " 274,\n",
       " 393,\n",
       " 184,\n",
       " 431,\n",
       " 87,\n",
       " 250,\n",
       " 736,\n",
       " 275,\n",
       " 266,\n",
       " 132,\n",
       " 151,\n",
       " 344,\n",
       " 334,\n",
       " 925,\n",
       " 249,\n",
       " 267,\n",
       " 581,\n",
       " 254,\n",
       " 270,\n",
       " 216,\n",
       " 370,\n",
       " 905,\n",
       " 237,\n",
       " 224,\n",
       " 224,\n",
       " 227,\n",
       " 237,\n",
       " 414,\n",
       " 229,\n",
       " 227,\n",
       " 227,\n",
       " 238,\n",
       " 227,\n",
       " 136,\n",
       " 103,\n",
       " 265,\n",
       " 157,\n",
       " 860,\n",
       " 484,\n",
       " 126,\n",
       " 744,\n",
       " 399,\n",
       " 109,\n",
       " 86,\n",
       " 245,\n",
       " 560,\n",
       " 106,\n",
       " 378,\n",
       " 423,\n",
       " 111,\n",
       " 928,\n",
       " 437,\n",
       " 99,\n",
       " 94,\n",
       " 109,\n",
       " 198,\n",
       " 102,\n",
       " 961,\n",
       " 297,\n",
       " 926,\n",
       " 276,\n",
       " 128,\n",
       " 441,\n",
       " 260,\n",
       " 755,\n",
       " 824,\n",
       " 953,\n",
       " 292,\n",
       " 294,\n",
       " 639,\n",
       " 447,\n",
       " 619,\n",
       " 620,\n",
       " 85,\n",
       " 121,\n",
       " 110,\n",
       " 240,\n",
       " 898,\n",
       " 125,\n",
       " 443,\n",
       " 449,\n",
       " 491,\n",
       " 396,\n",
       " 511,\n",
       " 322,\n",
       " 1005,\n",
       " 130,\n",
       " 294,\n",
       " 808,\n",
       " 599,\n",
       " 390,\n",
       " 370,\n",
       " 741,\n",
       " 455,\n",
       " 226,\n",
       " 229,\n",
       " 83,\n",
       " 798,\n",
       " 685,\n",
       " 807,\n",
       " 215,\n",
       " 655,\n",
       " 181,\n",
       " 956,\n",
       " 141,\n",
       " 224,\n",
       " 255,\n",
       " 274,\n",
       " 115,\n",
       " 290,\n",
       " 295,\n",
       " 222,\n",
       " 676,\n",
       " 735,\n",
       " 71,\n",
       " 73,\n",
       " 917,\n",
       " 53,\n",
       " 473,\n",
       " 346,\n",
       " 360,\n",
       " 450,\n",
       " 296,\n",
       " 282,\n",
       " 496,\n",
       " 822,\n",
       " 198,\n",
       " 468,\n",
       " 376,\n",
       " 270,\n",
       " 542,\n",
       " 710,\n",
       " 577,\n",
       " 222,\n",
       " 737,\n",
       " 933,\n",
       " 456,\n",
       " 589,\n",
       " 124,\n",
       " 412,\n",
       " 162,\n",
       " 831,\n",
       " 92,\n",
       " 156,\n",
       " 525,\n",
       " 560,\n",
       " 653,\n",
       " 210,\n",
       " 211,\n",
       " 104,\n",
       " 479,\n",
       " 984,\n",
       " 898,\n",
       " 390,\n",
       " 390,\n",
       " 459,\n",
       " 414,\n",
       " 849,\n",
       " 536,\n",
       " 397,\n",
       " 49,\n",
       " 347,\n",
       " 429,\n",
       " 588,\n",
       " 509,\n",
       " 143,\n",
       " 650,\n",
       " 333,\n",
       " 162,\n",
       " 711,\n",
       " 265,\n",
       " 271,\n",
       " 235,\n",
       " 308,\n",
       " 283,\n",
       " 176,\n",
       " 202,\n",
       " 116,\n",
       " 531,\n",
       " 379,\n",
       " 340,\n",
       " 233,\n",
       " 354,\n",
       " 395,\n",
       " 103,\n",
       " 101,\n",
       " 566,\n",
       " 448,\n",
       " 680,\n",
       " 427,\n",
       " 158,\n",
       " 708,\n",
       " 239,\n",
       " 631,\n",
       " 339,\n",
       " 457,\n",
       " 657,\n",
       " 940,\n",
       " 444,\n",
       " 754,\n",
       " 398,\n",
       " 439,\n",
       " 853,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m fullmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mamax(att_cpu, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      3\u001b[0m fullmean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(att_cpu, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39marray(fullmax))\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mcolorbar()\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "att_cpu = results['attentions'].squeeze().cpu()\n",
    "fullmax = torch.amax(att_cpu, dim=(0,1))\n",
    "fullmean = torch.mean(att_cpu, dim=(0,1))\n",
    "plt.imshow(np.array(fullmax))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.array(fullmean))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
